<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>redis on Wound:Abser&#39;s Blog</title>
    <link>https://blog.abser.top/tags/redis/</link>
    <description>Recent content in redis on Wound:Abser&#39;s Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Sun, 25 Nov 2018 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="https://blog.abser.top/tags/redis/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>HyperLogLog</title>
      <link>https://blog.abser.top/blog/hyperloglog%E7%AE%97%E6%B3%95/</link>
      <pubDate>Sun, 25 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>https://blog.abser.top/blog/hyperloglog%E7%AE%97%E6%B3%95/</guid>
      <description>更好的阅读参见我的语雀
基数计数基本概念 基数计数(cardinality counting)通常用来统计一个集合中不重复的元素个数，例如统计某个网站的UV，或者用户搜索网站的关键词数量。数据分析、网络监控及数据库优化等领域都会涉及到基数计数的需求。 要实现基数计数，最简单的做法是记录集合中所有不重复的元素集合，当新来一个元素，若中不包含元素​，则将加入，否则不加入，计数值就是​的元素数量。这种做法存在两个问题：
 当统计的数据量变大时，相应的存储内存也会线性增长 当集合变大，判断其是否包含新加入元素​的成本变大  概率算法 实际上目前还没有发现更好的在大数据场景中准确计算基数的高效算法，因此在不追求绝对准确的情况下，使用概率算法算是一个不错的解决方案。概率算法不直接存储数据集合本身，通过一定的概率统计方法预估基数值，这种方法可以大大节省内存，同时保证误差控制在一定范围内。目前用于基数计数的概率算法包括:
 Linear Counting(LC)：早期的基数估计算法，LC在空间复杂度方面并不算优秀，实际上LC的空间复杂度与简单bitmap方法是一样的（但是有个常数项级别的降低），都是O(N​max​​)； LogLog Counting(LLC)：LogLog Counting相比于LC更加节省内存，空间复杂度只有O(log​2​​(log​2​​(N​max​​))) HyperLogLog Counting(HLL)：HyperLogLog Counting是基于LLC的优化和改进，在同样空间复杂度情况下，能够比LLC的基数估计误差更小。  HLL 直观演示 HLLDEMO
HLL的实际步骤  通过hash函数计算输入值对应的比特串 比特串的低 位对应的数字用来找到数组S中对应的位置 i t+1位开始找到第一个1出现的位置 k，将 k 记入数组位置 基于数组S记录的所有数据的统计值，计算整体的基数值，计算公式可以简单表示为：  HLL是LLC的误差改进，实际是基于LLC。
算法来源（N次伯努利过程） 下面非正式的从直观角度描述LLC算法的思想来源。
设a为待估集合（哈希后）中的一个元素，由上面对H的定义可知，a可以看做一个长度固定的比特串（也就是a的二进制表示），设H哈希后的结果长度为L比特，我们将这L个比特位从左到右分别编号为1、2、…、L：
又因为a是从服从均与分布的样本空间中随机抽取的一个样本，因此a每个比特位服从如下分布且相互独立。
 通俗说就是a的每个比特位为0和1的概率各为0.5，且相互之间是独立的。 设 ρ(a)为a的比特串中第一个“1”出现的位置，显然1≤ρ(a)≤L，这里我们忽略比特串全为0的情况（概率为）。如果我们遍历集合中所有元素的比特串，取为所有ρ(a)的最大值。 此时我们可以将作为基数的一个粗糙估计，即：  解释 注意如下事实：
由于比特串每个比特都独立且服从0-1分布，因此从左到右扫描上述某个比特串寻找第一个“1”的过程从统计学角度看是一个伯努利过程，例如，可以等价看作不断投掷一个硬币（每次投掷正反面概率皆为0.5），直到得到一个正面的过程。在一次这样的过程中，投掷一次就得到正面的概率为1/2，投掷两次得到正面的概率是，…，投掷k次才得到第一个正面的概率为。
现在考虑如下两个问题：
1、进行n次伯努利过程，所有投掷次数都不大于k的概率是多少？
2、进行n次伯努利过程，至少有一次投掷次数等于k的概率是多少？
首先看第一个问题，在一次伯努利过程中，投掷次数大于k的概率为，即连续掷出k个反面的概率。因此，在一次过程中投掷次数不大于k的概率为。因此，n次伯努利过程投掷次数均不大于k的概率为：
 显然第二个问题的答案是：
 从以上分析可以看出，当时，Pn(X≥k)的概率几乎为0，同时，当时，Pn(X≤k)的概率也几乎为0。用自然语言概括上述结论就是：当伯努利过程次数远远小于时，至少有一次过程投掷次数等于k的概率几乎为0；当伯努利过程次数远远大于时，没有一次过程投掷次数大于k的概率也几乎为0。
如果将上面描述做一个对应：一次伯努利过程对应一个元素的比特串，反面对应0，正面对应1，投掷次数k对应第一个“1”出现的位置，我们就得到了下面结论：
设一个集合的基数为n，为所有元素中首个“1”的位置最大的那个元素的“1”的位置，如果n远远小于，则我们得到为当前值的概率几乎为0（它应该更小），同样的，如果n远远大于，则我们得到为当前值的概率也几乎为0（它应该更大），因此可以作为基数n的一个粗糙估计。
以上结论可以总结为：进行了n次进行抛硬币实验，每次分别记录下第一次抛到正面的抛掷次数kk，那么可以用n次实验中最大的抛掷次数来预估实验组数量n： 
回到基数统计的问题，我们需要统计一组数据中不重复元素的个数，集合中每个元素的经过hash函数后可以表示成0和1构成的二进制数串，一个二进制串可以类比为一次抛硬币实验，1是抛到正面，0是反面。二进制串中从低位开始第一个1出现的位置可以理解为抛硬币试验中第一次出现正面的抛掷次数k，那么基于上面的结论，我们可以通过多次抛硬币实验的最大抛到正面的次数来预估总共进行了多少次实验，同样可以可以通过第一个1出现位置的最大值​来预估总共有多少个不同的数字（整体基数）。
LogLogCounting 均匀随机化 与LC一样，在使用LLC之前需要选取一个哈希函数H应用于所有元素，然后对哈希值进行基数估计。H必须满足如下条件（定性的）：
1、H的结果具有很好的均匀性，也就是说无论原始集合元素的值分布如何，其哈希结果的值几乎服从均匀分布（完全服从均匀分布是不可能的，D. Knuth已经证明不可能通过一个哈希函数将一组不服从均匀分布的数据映射为绝对均匀分布，但是很多哈希函数可以生成几乎服从均匀分布的结果，这里我们忽略这种理论上的差异，认为哈希结果就是服从均匀分布）。
2、H的碰撞几乎可以忽略不计。也就是说我们认为对于不同的原始值，其哈希结果相同的概率非常小以至于可以忽略不计。</description>
    </item>
    
  </channel>
</rss>